{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this worksheets is part of the [mlvu machine learning course](https://mlvu.github.io)<br>\n",
    "setting up your environment: https://bit.ly/3bzpn5C\n",
    "\n",
    "# Worksheet 2: SciKit-learn\n",
    "\n",
    "SciKit is a large collection of python libraries for scientific computing. SciKit-learn, or sklearn is its Machine Learning library. It's very complete, and integrates well with other data science tools like Numpy, Matplotlib, Pandas and Keras.\n",
    "\n",
    "We'll start by importing it, together with numpy and matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "We'll start with a classification problem, based on the data we used in the previous worksheet. This time we've removed the living cricketers, and extended the data with a third column, indicating whether the cricketer was \"killed in action\" (i.e. they died in a war). \n",
    "\n",
    "Let's start by loading the data and having a look at it. Note that we are using the third column in the data for color. (the option ``cmap='RdYlBu_r`` just makes the scatter function use the right colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('cricketers.cls.csv', delimiter=',')\n",
    "plt.scatter(data[:,0], data[:,1], s=4, alpha=0.3, c=data[:,2], cmap='RdYlBu_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be our classification task: given the age of death and year of birth of a cricketer, predict whether they died in a war. Before we start, it's worth noting the following:\n",
    "* Our data has high class-imbalance. The proportion of cricketers killed in action is very low.\n",
    "* The target value has a natural relation to one of the features: if people died young, they're more likely to have been killed in action. \n",
    "*  The relationship with the other variable is also strong, but slightly arbitrary: we know that the years 1914-18 and 1939-1945 are special, but this is background knowlegde that a succesful classifier will have to memorize.\n",
    "\n",
    "First, we'll split our data into the features ``x`` and the target ``y``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[:,0:2]\n",
    "y = data[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll need to split the data into the training data (which we'll show to the learner) and the test data (which we'll hold back to see how well it has done). Sklearn provides a utility function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply an _estimator_ to the data. An estimator is anything that implements the method ``fit(x, y)`` which takes data and learns from it, and the method ``predict(T)`` which takes a new instance and predicts the target value.\n",
    "\n",
    "We'll start with a simple linear classifier. For reasons that will become clear later in the course, we do this by using the \"SVC model\" with a \"linear kernel.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "linear = SVC(kernel='linear')\n",
    "linear.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has now been fit to the training data. Let's make up some new cricketers, and see how it classifies them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw_data = np.asarray([[1896, 20],[1922, 20]])\n",
    "linear.predict(nw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier thinks both of them are non-KIA (even though they both died in the middle of a World War). Does this mean our classifier has poor accuracy? Let's [compute it](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_predicted = linear.predict(x_test)\n",
    "accuracy_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets 96% of the test correct. That doesn't seem so bad.\n",
    "\n",
    "Since we have a 2D feature space, we can visualize exactly what the classifier thinks. We simply color each point by what class the classifier assigns. The library ``mlxtend`` provides a utility function for this. We'll also plot the first 500 examples from the test set.\n",
    "\n",
    "This is a slow function, so you may have to wait a while for it to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "plot_decision_regions(x_test[:500], y_test.astype(np.int32)[:500], clf=linear);\n",
    "\n",
    "#You may get a warning about contour levels. Don't worry about that. Retry the cell if something goes wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The colors and shapes of the points indicate the true classes, the background color(s) indicate the predictions of the classifier.\n",
    "\n",
    "Clearly, the linear classifier just classifies everything as non-KIA. Due to the class imbalance, this still yields a high accuracy. Let's try another classifier, and see if it is flexible enough to isolate the small regions of feature space that correspond to the two world wars.\n",
    "\n",
    "We'll use a **decision tree** this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "y_predicted = tree.predict(x_test)\n",
    "accuracy_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit higher. Not a lot, but then, there wasn't a lot higher to go. Let's visualize the classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(x_test[:500], y_test.astype(np.int32)[:500], clf=tree);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It carves out some regions for the KIA class, but they're not very natural. Let's try one more classifier: k-Nearest Neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(15) # We set the number of neighbors to 15\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "y_predicted = knn.predict(x_test)\n",
    "print('accuracy ', accuracy_score(y_test, y_predicted))\n",
    "\n",
    "plot_decision_regions(x_test[:500], y_test.astype(np.integer)[:500], clf=knn); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves\n",
    "\n",
    "Clearly, the accuracy is not a very informative value with such a high class-imbalance. Instead, we can plot the ROC curve: we retrieve class probabilities form the models, and vary the decision threshold. We plot the number of false positives against the number of false negatives (see lecture 3 for more information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# The linear classifier doesn't produce class probabilities by default. We'll retrain it for probabilities.\n",
    "linear = SVC(kernel='linear', probability=True)\n",
    "linear.fit(x_train, y_train)\n",
    "\n",
    "# We'll need class probabilities from each of the classifiers\n",
    "y_linear = linear.predict_proba(x_test)\n",
    "y_tree  = tree.predict_proba(x_test)\n",
    "y_knn   = knn.predict_proba(x_test)\n",
    "\n",
    "# Compute the points on the curve\n",
    "# We pass the probability of the second class (KIA) as the y_score\n",
    "curve_linear = sklearn.metrics.roc_curve(y_test, y_linear[:, 1])\n",
    "curve_tree   = sklearn.metrics.roc_curve(y_test, y_tree[:, 1])\n",
    "curve_knn    = sklearn.metrics.roc_curve(y_test, y_knn[:, 1])\n",
    "\n",
    "# Compute Area Under the Curve\n",
    "auc_linear = auc(curve_linear[0], curve_linear[1])\n",
    "auc_tree   = auc(curve_tree[0], curve_tree[1])\n",
    "auc_knn    = auc(curve_knn[0], curve_knn[1])\n",
    "\n",
    "plt.plot(curve_linear[0], curve_linear[1], label='linear (area = %0.2f)' % auc_linear)\n",
    "plt.plot(curve_tree[0], curve_tree[1], label='tree (area = %0.2f)' % auc_tree)\n",
    "plt.plot(curve_knn[0], curve_knn[1], label='knn (area = %0.2f)'% auc_knn)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve');\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paints an interesting picture. If we look at the raw probabilities, the linear classifier almost always assigns non-KIA a higher probability than KIA. However, if we decide that we will allow a few false positives (non-KIA  classified as KIA), we can increase the true positive rate a lot. \n",
    "\n",
    "The decision tree classifier suffers here, because it computes its class probabilities from very small numbers of examples. This makes the number of different probabilities it can return smaller, which in turn means that we have less power to trade false positives for true positives, by varying the decision threshold. You can probably improve its performance by tweaking its hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "As we discussed in the lectures, cross-validation is a good way to make your estimates of accuracy more precise. However, it requires us to retrain the model many times on different splits of the data. This means that our training code looks a little different. \n",
    "\n",
    "We'll show briefly how it works for the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "\n",
    "# The cross_val_score function does all the training for us. We simply pass \n",
    "# it the complete data, the model, and the metric.\n",
    "\n",
    "linear = SVC(kernel='linear', probability=True)\n",
    "\n",
    "# Train for 5 folds, returing ROC AUC. You can also try 'accuracy' as a scorer\n",
    "scores = cross_val_score(linear, x, y, cv=5, scoring='roc_auc')\n",
    "\n",
    "print('scores per fold ', scores)\n",
    "print('  mean score    ', np.mean(scores))\n",
    "print('  standard dev. ', np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are using cross validation on the whole dataset here. If we use cross validation for hyperparameter selection, we should still withhold a test set and test our final choice of hyperparameters on the test set.\n",
    "\n",
    "## Regression\n",
    "\n",
    "**NB: We're re-using some variable names below. This means that if you want to re-run the classification experiments above after running these cells, you should do \"Kernel > Restart & Clear Output\" and start again from the top. Make sure always to run at least the first cell, so the right packages are imported.**\n",
    "\n",
    "Training regression models works very similar to training classification models. We'll try a quick example, to show the basics. Let's start with [one of the datasets that come pre-packaged with sklearn](https://scikit-learn.org/stable/datasets/toy_dataset.html). The data is described [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the diabetes dataset, and select one feature (Body Mass Index)\n",
    "x, y = datasets.load_diabetes(return_X_y=True)\n",
    "x = x[:, 2].reshape(-1, 1)\n",
    "\n",
    "# -- the reshape operation ensures that x still has two dimensions \n",
    "# (that is, we need it to be an n by 1 matrix, not a vector)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\n",
    "\n",
    "plt.scatter(x_train[:, 0], y_train)\n",
    "\n",
    "plt.xlabel('BMI')\n",
    "plt.ylabel('disease progression');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have only one feature here. The feature space is the horizontal axis and the _output space_ is the vertical axis.\n",
    "\n",
    "We'll assume that the basic idea is clear now, so we'll train three models in one go: linear regression, tree regression, and a knn regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "linear = LinearRegression()\n",
    "tree = DecisionTreeRegressor()\n",
    "knn = KNeighborsRegressor(10)\n",
    "\n",
    "linear.fit(x_train, y_train)\n",
    "tree.fit(x_train, y_train)\n",
    "knn.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot them, we'll just compute their outputs for a series of linearly spaced input points. We'll also plot the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "plt.scatter(x_train, y_train, alpha=0.1)\n",
    "\n",
    "xlin = np.linspace(-0.10, 0.2, 500).reshape(-1, 1)\n",
    "plt.plot(xlin, linear.predict(xlin), label='linear')\n",
    "plt.plot(xlin, tree.predict(xlin), label='tree ')\n",
    "plt.plot(xlin, knn.predict(xlin), label='knn ')\n",
    "\n",
    "print('MSE linear ', mean_squared_error(y_test, linear.predict(x_test)))\n",
    "print('MSE tree ', mean_squared_error(y_test, tree.predict(x_test)))\n",
    "print('MSE knn', mean_squared_error(y_test, knn.predict(x_test)))\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model would you choose? \n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "One thing we haven't looked at is _hyperparameters_. We've only used each model with its default settings. To show how crucial it is to check different hyperparameters, let's see how the number of neighbors influences the regression performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_test, y_test, alpha=0.1)\n",
    "\n",
    "xlin = np.linspace(-0.10, 0.2, 500).reshape(-1, 1)\n",
    "\n",
    "knn05 = KNeighborsRegressor(5)\n",
    "knn15 = KNeighborsRegressor(15)\n",
    "knn50 = KNeighborsRegressor(50)\n",
    "\n",
    "for model in [knn05, knn15, knn50]:\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "plt.plot(xlin, knn05.predict(xlin), label='knn 5')\n",
    "plt.plot(xlin, knn15.predict(xlin), label='knn 15')\n",
    "plt.plot(xlin, knn50.predict(xlin), label='knn 50')\n",
    "\n",
    "print('MSE knn  5 ', mean_squared_error(y_test, knn05.predict(x_test)))\n",
    "print('MSE knn 15 ', mean_squared_error(y_test, knn15.predict(x_test)))\n",
    "print('MSE knn 50 ', mean_squared_error(y_test, knn50.predict(x_test)))\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final comments\n",
    "\n",
    "This should give you a basic starting point to start experimenting with sklearn. As before, there is much more to explore. Here are some tutorials that dig a little deeper:\n",
    "\n",
    "* [The official quickstart guide](https://scikit-learn.org/1.4/tutorial/basic/tutorial.html)\n",
    "* [A DataCamp tutorial with interactive exercises](https://www.datacamp.com/community/tutorials/machine-learning-python)\n",
    "* [Analyzing text data with SKLearn](https://scikit-learn.org/1.4/tutorial/text_analytics/working_with_text_data.html)\n",
    "\n",
    "SKLearn may seem a little bit more complex than libraries like numpy and matplotlib, but that's because it requires a little knowledge of the underlying machine learning models. Once you start to understand the models better, you'll undertand the library better as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
